{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 학습 데이터 준비\n",
        " txt파일의 카톡 데이터를 모델 학습에 편리한 형식으로 바꿔주는 작업을 합니다."
      ],
      "metadata": {
        "id": "13Sku1lyDBQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "def katalk_msg_parse(file_path):\n",
        "    my_katalk_data = list()\n",
        "    katalk_msg_pattern = \"[0-9]{4}[년.] [0-9]{1,2}[월.] [0-9]{1,2}[일.] 오\\S [0-9]{1,2}:[0-9]{1,2},.*:\"\n",
        "    date_info = \"[0-9]{4}년 [0-9]{1,2}월 [0-9]{1,2}일 \\S요일\"\n",
        "    in_out_info = \"[0-9]{4}[년.] [0-9]{1,2}[월.] [0-9]{1,2}[일.] 오\\S [0-9]{1,2}:[0-9]{1,2}:.*\"\n",
        "\n",
        "    # uploaded is a dictionary, get the first file's content\n",
        "    file_content = list(uploaded.values())[0]\n",
        "    # Decode the content if it's in bytes and open it as a file-like object\n",
        "    decoded_content = io.StringIO(file_content.decode('utf-8'))\n",
        "\n",
        "    for line in decoded_content:  # Iterate over lines in the decoded content\n",
        "        if re.match(date_info, line) or re.match(in_out_info, line):\n",
        "            continue\n",
        "        elif line == '\\n':\n",
        "            continue\n",
        "        elif re.match(katalk_msg_pattern, line):\n",
        "            line = line.split(\",\")\n",
        "            date_time = line[0]\n",
        "            user_text = line[1].split(\" : \", maxsplit=1)\n",
        "            user_name = user_text[0].strip()\n",
        "            text = user_text[1].strip()\n",
        "            my_katalk_data.append({\n",
        "                                   'User': user_name,\n",
        "                                   'Message': text\n",
        "                                   })\n",
        "\n",
        "        else:\n",
        "            if len(my_katalk_data) > 0:\n",
        "                my_katalk_data[-1]['Message'] += \"\\n\"+line.strip()\n",
        "\n",
        "    my_katalk_df = pd.DataFrame(my_katalk_data)\n",
        "\n",
        "    return my_katalk_df\n",
        "\n",
        "df=katalk_msg_parse(uploaded)"
      ],
      "metadata": {
        "id": "ElSO6oeQ0fq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "변경된 카톡 데이터"
      ],
      "metadata": {
        "id": "J6WTcBfVDN_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(df)"
      ],
      "metadata": {
        "id": "_bQt53Rp2Xdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 형식 재조정\n",
        "\n",
        "'나'와 '상대방'의 대화를 학습시켜야 하기 때문에\n",
        "'me'와 'you'라는 형식의 데이터로 재조정합니다."
      ],
      "metadata": {
        "id": "iaL3sYRTDUOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = pd.DataFrame(columns=['User', 'Message'])\n",
        "current_user = None\n",
        "current_message = \"\"\n",
        "\n",
        "# 같은 User 가 연속으로 나타날 경우 한개의 Row로 처리\n",
        "for index, row in df.iterrows():\n",
        "    if current_user is None:\n",
        "        current_user = row['User']\n",
        "        current_message += row['Message']\n",
        "    elif current_user != row['User']:\n",
        "        # new_df = new_df.append({'User': current_user, 'Message': current_message}, ignore_index=True) 대신 아래 코드 사용\n",
        "        new_df = pd.concat([new_df, pd.DataFrame([{'User': current_user, 'Message': current_message}])], ignore_index=True)\n",
        "        current_user = row['User']\n",
        "        current_message = row['Message']\n",
        "    else:\n",
        "        current_message += \"\\n\" + row['Message']\n",
        "\n",
        "# 마지막 행 추가\n",
        "if current_user is not None:\n",
        "    # new_df = new_df.append({'User': current_user, 'Message': current_message}, ignore_index=True) 대신 아래 코드 사용\n",
        "    new_df = pd.concat([new_df, pd.DataFrame([{'User': current_user, 'Message': current_message}])], ignore_index=True)\n",
        "\n",
        "#\n",
        "me = new_df[new_df.User =='김도겸'].reset_index().Message\n",
        "you = new_df[new_df.User =='김회훈'].reset_index().Message\n",
        "\n",
        "data = pd.DataFrame({'me':me , 'you' : you})"
      ],
      "metadata": {
        "id": "tm4ewq3Q0u9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 파인튜닝(미세조정)\n",
        "\n",
        "모델을 내 대화에 맞게 조정하는 작업을 해봅시다."
      ],
      "metadata": {
        "id": "I-_UvyRk0k7T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8xZJPQZ0Pon"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "checkpoint = 'skt/kogpt2-base-v2'\n",
        "device = 'cuda'\n",
        "\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(checkpoint,\n",
        "  bos_token='<s>', eos_token='</s>', unk_token='<unk>',\n",
        "  pad_token='<pad>', mask_token='<mask>')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습을 위한 데이터셋 만들기\n",
        "\n",
        "학습 데이터셋을 만들기 위한 라이브러리를 설치합니다."
      ],
      "metadata": {
        "id": "pzGMSZW_CZxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Dpmoubr14oNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "dataset = Dataset.from_pandas(data).map(\n",
        "    lambda x: {'text': f\"### Me: {x['me']}\\n### You: {x['you']}</s>\" } #end token </s>\n",
        ").train_test_split(test_size=0.1)\n",
        "\n",
        "#kogpt2 의 경우 max length 가 1024로 1024 이상의 길이는 Truncate 해야줘야 합니다.\n",
        "dataset = dataset.map(lambda x: tokenizer(x[\"text\"] , truncation=True, max_length=1024, padding=\"max_length\"), batched=True)"
      ],
      "metadata": {
        "id": "CfMoLaGJ0SE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 불러오기\n",
        "KoGPT2 모델을 사용해서 실습을 진행합니다.\n",
        "https://github.com/SKT-AI/KoGPT2"
      ],
      "metadata": {
        "id": "Ni7JTXjkCHQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "model = GPT2LMHeadModel.from_pretrained(checkpoint).to(device)"
      ],
      "metadata": {
        "id": "JS62x3Yi6230"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "# Tokenizer 설정\n",
        "if tokenizer.pad_token is None:\n",
        "    if tokenizer.eos_token is not None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    else:\n",
        "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# TrainingArguments 설정\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"test_trainer\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=3,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    learning_rate=2e-5,\n",
        "    optim=\"adamw_torch\",\n",
        "    fp16=True,  # GPU 지원 시\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=100,\n",
        ")\n",
        "\n",
        "# Trainer 설정\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['test'],\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "\n",
        "# 학습 시작\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "DYIglBhi0Ud0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "재조정된 모델에 입력을 도와주는 함수"
      ],
      "metadata": {
        "id": "JcXhHk9I0YvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_model(prompt: str, model, tokenizer, device='cuda', max_length=32) -> str:\n",
        "    input_text = f\"### Me: {prompt}\\n### You: \"\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        input_text,\n",
        "        return_tensors='pt',\n",
        "        return_token_type_ids=False\n",
        "    ).to(device)\n",
        "\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_length=max_length,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        bos_token_id=tokenizer.bos_token_id,\n",
        "        use_cache=True,\n",
        "        early_stopping=True,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=30,\n",
        "        top_p=0.80\n",
        "    )\n",
        "\n",
        "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # '### You: ' 이후 텍스트 추출\n",
        "    if '### You:' in decoded:\n",
        "        return decoded.split('### You:')[1].strip()\n",
        "    else:\n",
        "        return decoded.strip()"
      ],
      "metadata": {
        "id": "eV0qgAVf0Zq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 이제는 직접 출력을 해봅시다!"
      ],
      "metadata": {
        "id": "YSJkKyEyBxkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat_with_model(\"카페갈건데 뭐 마실래??\", model, tokenizer)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "5ycHF09HBs_W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}